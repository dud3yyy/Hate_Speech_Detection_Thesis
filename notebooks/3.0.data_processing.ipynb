{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arbru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arbru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arbru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from contractions import fix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.read_csv('../data/annotated/dehatebert/classified_x_df_CNERG.csv')\n",
    "x_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {x_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def censor_words(text, banned_words):\n",
    "    def censor_match(match):\n",
    "        word = match.group(0)\n",
    "        censored_word = re.sub(r'([aeiouAEIOU])', '*', word, count=1)\n",
    "        return censored_word\n",
    "\n",
    "    pattern = r\"\\b(\" + \"|\".join(re.escape(word) for word in banned_words) + r\")\\b\"\n",
    "    return re.sub(pattern, censor_match, text, flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(x_df[\"Full_Text\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,  \n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\" \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"X Word Cloud Before Text Processing\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function that removes @, special characters or hashtags.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordsegment import load, segment\n",
    "\n",
    "load()\n",
    "\n",
    "def fix_hashtags(text):\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.startswith(\"#\"):\n",
    "            clean_word = word[1:]  \n",
    "            \n",
    "            if re.search(r'[A-Z]', clean_word):\n",
    "                clean_word = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', clean_word)\n",
    "            else:\n",
    "                clean_word = \" \".join(segment(clean_word))\n",
    "            \n",
    "            processed_words.append(clean_word)\n",
    "        else:\n",
    "            processed_words.append(word)\n",
    "\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"cleaned_text\"] = x_df[\"Full_Text\"].astype(str).apply(clean_text)\n",
    "x_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"cleaned_text\"] = x_df[\"cleaned_text\"].astype(str).apply(fix_hashtags)\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"cleaned_text\"] = x_df[\"cleaned_text\"].apply(fix)\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"tokens\"] = x_df[\"cleaned_text\"].apply(word_tokenize)\n",
    "print(x_df[[\"cleaned_text\", \"tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "x_df[\"tokens\"] = x_df[\"tokens\"].apply(lambda words: [w for w in words if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "x_df[\"tokens\"] = x_df[\"tokens\"].apply(lambda words: [lemmatizer.lemmatize(w) for w in words])\n",
    "x_df[\"processed_text\"] = x_df[\"tokens\"].apply(lambda words: ' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[[\"processed_text\", \"label\"]].to_csv(\"../data/processed/processed_x_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(x_df[\"processed_text\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,\n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\"   \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"X Word Cloud After Text Processing\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = x_df['label'].value_counts()\n",
    "plt.figure(figsize=(3,3))\n",
    "bars = class_count.plot(kind='bar', color=['lightgreen','orange','red'])\n",
    "\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, \n",
    "             bar.get_height(), \n",
    "             f\"{bar.get_height():,}\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "    \n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Distribution for the X Dataset (dehatebert)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46638 entries, 0 to 46637\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      46638 non-null  int64  \n",
      " 1   title           46638 non-null  object \n",
      " 2   body            19125 non-null  object \n",
      " 3   author          46638 non-null  object \n",
      " 4   comment         46638 non-null  object \n",
      " 5   comment_author  38974 non-null  object \n",
      " 6   score           46638 non-null  int64  \n",
      " 7   upvote_ratio    46638 non-null  float64\n",
      " 8   created_utc     46638 non-null  object \n",
      " 9   subreddit       46638 non-null  object \n",
      " 10  label           46638 non-null  object \n",
      " 11  confidence      46638 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(8)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "reddit_df = pd.read_csv(\"../data/annotated/dehatebert/classified_reddit_df_CNERG.csv\")\n",
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {reddit_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(reddit_df[\"comment\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,  \n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\" \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Reddit Word Cloud Before Text Processing\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"cleaned_text\"] = reddit_df[\"comment\"].astype(str).apply(clean_text)\n",
    "reddit_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"cleaned_text\"] = reddit_df[\"cleaned_text\"].astype(str).apply(fix_hashtags)\n",
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"cleaned_text\"] = reddit_df[\"cleaned_text\"].apply(fix)\n",
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"tokens\"] = reddit_df[\"cleaned_text\"].apply(word_tokenize)\n",
    "print(reddit_df[[\"cleaned_text\", \"tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "reddit_df[\"tokens\"] = reddit_df[\"tokens\"].apply(lambda words: [w for w in words if w not in stop_words])\n",
    "reddit_df[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "reddit_df[\"tokens\"] = reddit_df[\"tokens\"].apply(lambda words: [lemmatizer.lemmatize(w) for w in words])\n",
    "reddit_df[\"processed_text\"] = reddit_df[\"tokens\"].apply(lambda words: ' '.join(words))\n",
    "reddit_df[\"processed_text\"].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_removed = [\"removed\",\"deleted\"]\n",
    "\n",
    "bad_values = reddit_df[\"processed_text\"].value_counts().loc[to_be_removed]\n",
    "plt.figure(figsize=(7,5))\n",
    "bars = bad_values.plot(kind='bar', color=['lightblue','orange'])\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, \n",
    "             bar.get_height(), \n",
    "             f\"{bar.get_height():,}\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "plt.xlabel('Type of record')\n",
    "plt.ylabel('Count')\n",
    "plt.title('No. of rows scraped from Reddit that have been removed or deleted')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[~reddit_df[\"processed_text\"].isin(to_be_removed)]\n",
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(reddit_df[\"comment\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,  \n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\" \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Reddit Word Cloud After Text Processing\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_class = reddit_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "reddit_bar = reddit_class.plot(kind='bar', color=['lightgreen','orange','red'])\n",
    "for bar in reddit_bar.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, \n",
    "             bar.get_height(), \n",
    "             f\"{bar.get_height():,}\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Class Distribution for the Reddit Dataset (dehatebert)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[['processed_text', 'label']].to_csv('../data/processed/processed_reddit_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = pd.read_csv('../data/HateSpeechDatasetBalanced.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726119\n",
      "Length w/o duplicates 700067\n"
     ]
    }
   ],
   "source": [
    "print(len(k_df))\n",
    "k_df.drop_duplicates(subset='Content', inplace=True)\n",
    "print(f\"Length w/o duplicates {len(k_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>726114</th>\n",
       "      <td>i mute this telecasting and played kanye west ...</td>\n",
       "      <td>1</td>\n",
       "      <td>i mute this telecasting and played kanye west ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726115</th>\n",
       "      <td>but hell yeah he s not a bachelor but looooooo...</td>\n",
       "      <td>1</td>\n",
       "      <td>but hell yeah he s not a bachelor but looooooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726116</th>\n",
       "      <td>great video musician but s not my musician lol...</td>\n",
       "      <td>1</td>\n",
       "      <td>great video musician but s not my musician lol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726117</th>\n",
       "      <td>not great pop video yeah he s not a pedophile ...</td>\n",
       "      <td>1</td>\n",
       "      <td>not great pop video yeah he s not a pedophile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726118</th>\n",
       "      <td>great video yeah he s non a paedophile lolllll...</td>\n",
       "      <td>1</td>\n",
       "      <td>great video yeah he s non a paedophile lolllll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Content  Label  \\\n",
       "726114  i mute this telecasting and played kanye west ...      1   \n",
       "726115  but hell yeah he s not a bachelor but looooooo...      1   \n",
       "726116  great video musician but s not my musician lol...      1   \n",
       "726117  not great pop video yeah he s not a pedophile ...      1   \n",
       "726118  great video yeah he s non a paedophile lolllll...      1   \n",
       "\n",
       "                                             cleaned_text  \n",
       "726114  i mute this telecasting and played kanye west ...  \n",
       "726115  but hell yeah he s not a bachelor but looooooo...  \n",
       "726116  great video musician but s not my musician lol...  \n",
       "726117  not great pop video yeah he s not a pedophile ...  \n",
       "726118  great video yeah he s non a paedophile lolllll...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df[\"cleaned_text\"] = k_df[\"Content\"].astype(str).apply(clean_text)\n",
    "k_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "      <td>afro american basho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Label  \\\n",
       "0  denial of normal the con be asked to comment o...      1   \n",
       "1  just by being able to tweet this insufferable ...      1   \n",
       "2  that is retarded you too cute to be single tha...      1   \n",
       "3  thought of a real badass mongol style declarat...      1   \n",
       "4                                afro american basho      1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  denial of normal the con be asked to comment o...  \n",
       "1  just by being able to tweet this insufferable ...  \n",
       "2  that is retarded you too cute to be single tha...  \n",
       "3  thought of a real badass mongol style declarat...  \n",
       "4                                afro american basho  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df[\"cleaned_text\"] = k_df[\"Content\"].astype(str).apply(fix_hashtags)\n",
    "k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "      <td>afro american basho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726114</th>\n",
       "      <td>i mute this telecasting and played kanye west ...</td>\n",
       "      <td>1</td>\n",
       "      <td>i mute this telecasting and played kanye west ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726115</th>\n",
       "      <td>but hell yeah he s not a bachelor but looooooo...</td>\n",
       "      <td>1</td>\n",
       "      <td>but hell yeah he s not a bachelor but looooooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726116</th>\n",
       "      <td>great video musician but s not my musician lol...</td>\n",
       "      <td>1</td>\n",
       "      <td>great video musician but s not my musician lol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726117</th>\n",
       "      <td>not great pop video yeah he s not a pedophile ...</td>\n",
       "      <td>1</td>\n",
       "      <td>not great pop video yeah he s not a pedophile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726118</th>\n",
       "      <td>great video yeah he s non a paedophile lolllll...</td>\n",
       "      <td>1</td>\n",
       "      <td>great video yeah he s non a paedophile lolllll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700067 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Content  Label  \\\n",
       "0       denial of normal the con be asked to comment o...      1   \n",
       "1       just by being able to tweet this insufferable ...      1   \n",
       "2       that is retarded you too cute to be single tha...      1   \n",
       "3       thought of a real badass mongol style declarat...      1   \n",
       "4                                     afro american basho      1   \n",
       "...                                                   ...    ...   \n",
       "726114  i mute this telecasting and played kanye west ...      1   \n",
       "726115  but hell yeah he s not a bachelor but looooooo...      1   \n",
       "726116  great video musician but s not my musician lol...      1   \n",
       "726117  not great pop video yeah he s not a pedophile ...      1   \n",
       "726118  great video yeah he s non a paedophile lolllll...      1   \n",
       "\n",
       "                                             cleaned_text  \n",
       "0       denial of normal the con be asked to comment o...  \n",
       "1       just by being able to tweet this insufferable ...  \n",
       "2       that is retarded you too cute to be single tha...  \n",
       "3       thought of a real badass mongol style declarat...  \n",
       "4                                     afro american basho  \n",
       "...                                                   ...  \n",
       "726114  i mute this telecasting and played kanye west ...  \n",
       "726115  but hell yeah he s not a bachelor but looooooo...  \n",
       "726116  great video musician but s not my musician lol...  \n",
       "726117  not great pop video yeah he s not a pedophile ...  \n",
       "726118  great video yeah he s non a paedophile lolllll...  \n",
       "\n",
       "[700067 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df[\"cleaned_text\"] = k_df[\"Content\"].apply(fix)\n",
    "k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0  denial of normal the con be asked to comment o...   \n",
      "1  just by being able to tweet this insufferable ...   \n",
      "2  that is retarded you too cute to be single tha...   \n",
      "3  thought of a real badass mongol style declarat...   \n",
      "4                                afro american basho   \n",
      "\n",
      "                                              tokens  \n",
      "0  [denial, of, normal, the, con, be, asked, to, ...  \n",
      "1  [just, by, being, able, to, tweet, this, insuf...  \n",
      "2  [that, is, retarded, you, too, cute, to, be, s...  \n",
      "3  [thought, of, a, real, badass, mongol, style, ...  \n",
      "4                            [afro, american, basho]  \n"
     ]
    }
   ],
   "source": [
    "k_df[\"tokens\"] = k_df[\"cleaned_text\"].apply(word_tokenize)\n",
    "print(k_df[[\"cleaned_text\", \"tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "k_df[\"tokens\"] = k_df[\"tokens\"].apply(lambda words: [w for w in words if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "k_df[\"tokens\"] = k_df[\"tokens\"].apply(lambda words: [lemmatizer.lemmatize(w) for w in words])\n",
    "k_df[\"processed_text\"] = k_df[\"tokens\"].apply(lambda words: ' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df.rename(columns={'Label':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>1</td>\n",
       "      <td>denial of normal the con be asked to comment o...</td>\n",
       "      <td>[denial, normal, con, asked, comment, tragedy,...</td>\n",
       "      <td>denial normal con asked comment tragedy emotio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just by being able to tweet this insufferable ...</td>\n",
       "      <td>[able, tweet, insufferable, bullshit, prof, tr...</td>\n",
       "      <td>able tweet insufferable bullshit prof trump na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>that is retarded you too cute to be single tha...</td>\n",
       "      <td>[retarded, cute, single, life]</td>\n",
       "      <td>retarded cute single life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>1</td>\n",
       "      <td>thought of a real badass mongol style declarat...</td>\n",
       "      <td>[thought, real, badass, mongol, style, declara...</td>\n",
       "      <td>thought real badass mongol style declaration w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afro american basho</td>\n",
       "      <td>1</td>\n",
       "      <td>afro american basho</td>\n",
       "      <td>[afro, american, basho]</td>\n",
       "      <td>afro american basho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726114</th>\n",
       "      <td>i mute this telecasting and played kanye west ...</td>\n",
       "      <td>1</td>\n",
       "      <td>i mute this telecasting and played kanye west ...</td>\n",
       "      <td>[mute, telecasting, played, kanye, west, cliqu...</td>\n",
       "      <td>mute telecasting played kanye west clique know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726115</th>\n",
       "      <td>but hell yeah he s not a bachelor but looooooo...</td>\n",
       "      <td>1</td>\n",
       "      <td>but hell yeah he s not a bachelor but looooooo...</td>\n",
       "      <td>[hell, yeah, not, bachelor, looooooooooooooooo...</td>\n",
       "      <td>hell yeah not bachelor loooooooooooooooooooooo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726116</th>\n",
       "      <td>great video musician but s not my musician lol...</td>\n",
       "      <td>1</td>\n",
       "      <td>great video musician but s not my musician lol...</td>\n",
       "      <td>[great, video, musician, not, musician, lollll...</td>\n",
       "      <td>great video musician not musician lollllllllll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726117</th>\n",
       "      <td>not great pop video yeah he s not a pedophile ...</td>\n",
       "      <td>1</td>\n",
       "      <td>not great pop video yeah he s not a pedophile ...</td>\n",
       "      <td>[not, great, pop, video, yeah, not, pedophile,...</td>\n",
       "      <td>not great pop video yeah not pedophile yeah lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726118</th>\n",
       "      <td>great video yeah he s non a paedophile lolllll...</td>\n",
       "      <td>1</td>\n",
       "      <td>great video yeah he s non a paedophile lolllll...</td>\n",
       "      <td>[great, video, yeah, non, paedophile, lollllll...</td>\n",
       "      <td>great video yeah non paedophile lollllllllllll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700067 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Content  label  \\\n",
       "0       denial of normal the con be asked to comment o...      1   \n",
       "1       just by being able to tweet this insufferable ...      1   \n",
       "2       that is retarded you too cute to be single tha...      1   \n",
       "3       thought of a real badass mongol style declarat...      1   \n",
       "4                                     afro american basho      1   \n",
       "...                                                   ...    ...   \n",
       "726114  i mute this telecasting and played kanye west ...      1   \n",
       "726115  but hell yeah he s not a bachelor but looooooo...      1   \n",
       "726116  great video musician but s not my musician lol...      1   \n",
       "726117  not great pop video yeah he s not a pedophile ...      1   \n",
       "726118  great video yeah he s non a paedophile lolllll...      1   \n",
       "\n",
       "                                             cleaned_text  \\\n",
       "0       denial of normal the con be asked to comment o...   \n",
       "1       just by being able to tweet this insufferable ...   \n",
       "2       that is retarded you too cute to be single tha...   \n",
       "3       thought of a real badass mongol style declarat...   \n",
       "4                                     afro american basho   \n",
       "...                                                   ...   \n",
       "726114  i mute this telecasting and played kanye west ...   \n",
       "726115  but hell yeah he s not a bachelor but looooooo...   \n",
       "726116  great video musician but s not my musician lol...   \n",
       "726117  not great pop video yeah he s not a pedophile ...   \n",
       "726118  great video yeah he s non a paedophile lolllll...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       [denial, normal, con, asked, comment, tragedy,...   \n",
       "1       [able, tweet, insufferable, bullshit, prof, tr...   \n",
       "2                          [retarded, cute, single, life]   \n",
       "3       [thought, real, badass, mongol, style, declara...   \n",
       "4                                 [afro, american, basho]   \n",
       "...                                                   ...   \n",
       "726114  [mute, telecasting, played, kanye, west, cliqu...   \n",
       "726115  [hell, yeah, not, bachelor, looooooooooooooooo...   \n",
       "726116  [great, video, musician, not, musician, lollll...   \n",
       "726117  [not, great, pop, video, yeah, not, pedophile,...   \n",
       "726118  [great, video, yeah, non, paedophile, lollllll...   \n",
       "\n",
       "                                           processed_text  \n",
       "0       denial normal con asked comment tragedy emotio...  \n",
       "1       able tweet insufferable bullshit prof trump na...  \n",
       "2                               retarded cute single life  \n",
       "3       thought real badass mongol style declaration w...  \n",
       "4                                     afro american basho  \n",
       "...                                                   ...  \n",
       "726114  mute telecasting played kanye west clique know...  \n",
       "726115  hell yeah not bachelor loooooooooooooooooooooo...  \n",
       "726116  great video musician not musician lollllllllll...  \n",
       "726117  not great pop video yeah not pedophile yeah lo...  \n",
       "726118  great video yeah non paedophile lollllllllllll...  \n",
       "\n",
       "[700067 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df[[\"processed_text\", \"label\"]].to_csv(\"../data/processed/processed_kaggle_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
