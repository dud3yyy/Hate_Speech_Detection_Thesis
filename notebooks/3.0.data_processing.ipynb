{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from contractions import fix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.read_csv('../data/annotated/dehatebert/classified_x_df_CNERG.csv')\n",
    "x_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {x_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def censor_words(text, banned_words):\n",
    "    def censor_match(match):\n",
    "        word = match.group(0)\n",
    "        censored_word = re.sub(r'([aeiouAEIOU])', '*', word, count=1)\n",
    "        return censored_word\n",
    "\n",
    "    pattern = r\"\\b(\" + \"|\".join(re.escape(word) for word in banned_words) + r\")\\b\"\n",
    "    return re.sub(pattern, censor_match, text, flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(x_df[\"Full_Text\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,  \n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\" \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"X Word Cloud Before Text Processing\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function that removes @, special characters or hashtags.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordsegment import load, segment\n",
    "\n",
    "load()\n",
    "\n",
    "def fix_hashtags(text):\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.startswith(\"#\"):\n",
    "            clean_word = word[1:]  \n",
    "            \n",
    "            if re.search(r'[A-Z]', clean_word):\n",
    "                clean_word = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', clean_word)\n",
    "            else:\n",
    "                clean_word = \" \".join(segment(clean_word))\n",
    "            \n",
    "            processed_words.append(clean_word)\n",
    "        else:\n",
    "            processed_words.append(word)\n",
    "\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"cleaned_text\"] = x_df[\"Full_Text\"].astype(str).apply(clean_text)\n",
    "x_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"cleaned_text\"] = x_df[\"cleaned_text\"].astype(str).apply(fix_hashtags)\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"cleaned_text\"] = x_df[\"cleaned_text\"].apply(fix)\n",
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[\"tokens\"] = x_df[\"cleaned_text\"].apply(word_tokenize)\n",
    "print(x_df[[\"cleaned_text\", \"tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "x_df[\"tokens\"] = x_df[\"tokens\"].apply(lambda words: [w for w in words if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "x_df[\"tokens\"] = x_df[\"tokens\"].apply(lambda words: [lemmatizer.lemmatize(w) for w in words])\n",
    "x_df[\"processed_text\"] = x_df[\"tokens\"].apply(lambda words: ' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df[[\"processed_text\", \"label\"]].to_csv(\"../data/processed/processed_x_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(x_df[\"processed_text\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,\n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\"   \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"X Word Cloud After Text Processing\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = x_df['label'].value_counts()\n",
    "plt.figure(figsize=(3,3))\n",
    "bars = class_count.plot(kind='bar', color=['lightgreen','orange','red'])\n",
    "\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, \n",
    "             bar.get_height(), \n",
    "             f\"{bar.get_height():,}\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "    \n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Distribution for the X Dataset (dehatebert)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(\"../data/annotated/dehatebert/classified_reddit_df_CNERG.csv\")\n",
    "reddit_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: {reddit_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(reddit_df[\"comment\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,  \n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\" \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Reddit Word Cloud Before Text Processing\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"cleaned_text\"] = reddit_df[\"comment\"].astype(str).apply(clean_text)\n",
    "reddit_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"cleaned_text\"] = reddit_df[\"cleaned_text\"].astype(str).apply(fix_hashtags)\n",
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"cleaned_text\"] = reddit_df[\"cleaned_text\"].apply(fix)\n",
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[\"tokens\"] = reddit_df[\"cleaned_text\"].apply(word_tokenize)\n",
    "print(reddit_df[[\"cleaned_text\", \"tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "reddit_df[\"tokens\"] = reddit_df[\"tokens\"].apply(lambda words: [w for w in words if w not in stop_words])\n",
    "reddit_df[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "reddit_df[\"tokens\"] = reddit_df[\"tokens\"].apply(lambda words: [lemmatizer.lemmatize(w) for w in words])\n",
    "reddit_df[\"processed_text\"] = reddit_df[\"tokens\"].apply(lambda words: ' '.join(words))\n",
    "reddit_df[\"processed_text\"].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_removed = [\"removed\",\"deleted\"]\n",
    "\n",
    "bad_values = reddit_df[\"processed_text\"].value_counts().loc[to_be_removed]\n",
    "plt.figure(figsize=(7,5))\n",
    "bars = bad_values.plot(kind='bar', color=['lightblue','orange'])\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, \n",
    "             bar.get_height(), \n",
    "             f\"{bar.get_height():,}\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "plt.xlabel('Type of record')\n",
    "plt.ylabel('Count')\n",
    "plt.title('No. of rows scraped from Reddit that have been removed or deleted')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df[~reddit_df[\"processed_text\"].isin(to_be_removed)]\n",
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "text_data = \" \".join(reddit_df[\"comment\"].astype(str))\n",
    "banned_words = {\"fuck\", \"fucking\", \"shit\", \"bitch\", \"faggot\", \"nigga\"}\n",
    "\n",
    "censored_text = censor_words(text_data, banned_words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color=\"black\",\n",
    "    colormap=\"viridis\",\n",
    "    max_words=200,  \n",
    "    regexp=r\"\\b[a-zA-Z*]+\\b\" \n",
    ").generate(censored_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Reddit Word Cloud After Text Processing\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_class = reddit_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "reddit_bar = reddit_class.plot(kind='bar', color=['lightgreen','orange','red'])\n",
    "for bar in reddit_bar.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, \n",
    "             bar.get_height(), \n",
    "             f\"{bar.get_height():,}\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Class Distribution for the Reddit Dataset (dehatebert)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[['processed_text', 'label']].to_csv('../data/processed/processed_reddit_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = pd.read_csv('../data/HateSpeechDatasetBalanced.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(k_df))\n",
    "k_df.drop_duplicates(subset='Content', inplace=True)\n",
    "print(f\"Length w/o duplicates {len(k_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df[\"cleaned_text\"] = k_df[\"Content\"].astype(str).apply(clean_text)\n",
    "k_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df[\"cleaned_text\"] = k_df[\"Content\"].astype(str).apply(fix_hashtags)\n",
    "k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df[\"cleaned_text\"] = k_df[\"Content\"].apply(fix)\n",
    "k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df[\"tokens\"] = k_df[\"cleaned_text\"].apply(word_tokenize)\n",
    "print(k_df[[\"cleaned_text\", \"tokens\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "k_df[\"tokens\"] = k_df[\"tokens\"].apply(lambda words: [w for w in words if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "k_df[\"tokens\"] = k_df[\"tokens\"].apply(lambda words: [lemmatizer.lemmatize(w) for w in words])\n",
    "k_df[\"processed_text\"] = k_df[\"tokens\"].apply(lambda words: ' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df.rename(columns={'Label':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df[[\"processed_text\", \"label\"]].to_csv(\"../data/processed/processed_kaggle_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
